# Read





## **Extending Machine Language Models toward Human-Level Language Understanding**

recent progress in this field depends on query-based attention, which extends the ability of these systems to exploit con- text and has contributed to remarkable breakthroughs.

We argue that progress has come from exploiting principles of neural computation employed by the human brain, while a key limitation is that these systems treat language as if it can stand alone. We propose that language works in concert with other inputs to understand and communicate about situations. We describe key aspects of human understanding and key components of the brainâ€™s understanding system. We then propose initial steps toward a model informed both by cognitive neuroscience and artificial intelligence and point to extensions addressing more abstract cases.

**Principles of Neural Computation**

- The principles of neural computation are domain general, inspired by the human brain and human abilities
- Another key principle is *mutual constraint satisfaction*

**Neural Language Modeling**

- Initial steps: RNN
- Scaling up to natural text: Embeddings. LSTM
- Query-based attention: BERT, GPT-3

**Language in an Integrated Understanding System**

- Situations
- People construct situation representations.
- The compositionality of situations
- Language informs us about situations

**Toward a Brain and AI Inspired Model of Understanding**

- The understanding system in the brain
  - Object representations
  - Representation of context
  - The role of language
  - Complementary learning systems
- Next steps toward a brain and AI-inspired model
- Enhancing understanding by incorporating interaction with the physical and social world

**Conclusion**

Language does not stand alone. The understanding system in the brain connects language to representations of objects and situations and enhances language understanding by exploiting the full range of our multi-sensory experience of the world, our representations of our motor actions, and our memory of previous situations. We believe next generation language understanding systems should emulate this system and we have sketched an approach that incorporates recent machine learning breakthroughs to build a jointly brain and AI inspired understanding system. We emphasize understanding of concrete situations and argue that understanding abstract language should build upon this foundation, pointing toward the possibility of one day building artificial systems that understand abstract situations far beyond concrete, here-and-now situations. In sum, combining insights from neuroscience and AI will take us closer to human-level language understanding.