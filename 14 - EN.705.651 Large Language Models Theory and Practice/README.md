# About this course

## Course



## General

## Textbook

## Grade

1. 30%: Paper Presentations/Discussion Leadership
2. 30%: Preparation and Participation 
3. 40%: Course Project

# Course Topics

 1. Introduction: The Structure, Statistics, and Representation of Language
2. Introduction to the Modeling of Language
3. What Is Attention?
4. The Transformer: All You Need Is Attention
5. The First Large Language Models -- Finally, Good Text Generation!
6. GPT-3, Meta-Learning, and Prompting
7. Prompting, Continued
8. Composability and Chaining
9. ReAct -- Agentive LLM Systems
10. Types of Tuning
11. Efficiency Through Software
12. Efficiency Through Hardware
13. Final Projects Pt. I
14. Final Projects Pt. II

# Check List

Discussion

Office Hours

Course

# Paper and article

## Resources: 

[Anti-hype LLM reading list](https://gist.github.com/veekaybee/be375ab33085102f9027853128dc5f0e)

Awesome

## Papers - Before Transformer

2000: [A Neural Probabilistic Language Model](https://proceedings.neurips.cc/paper_files/paper/2000/file/728f206c2a01bf572b5940d7d9a8fa4c-Paper.pdf)

2015: [From Feedforward to Recurrent LSTM Neural Networks for Language Modeling](https://proceedings.neurips.cc/paper_files/paper/2000/file/728f206c2a01bf572b5940d7d9a8fa4c-Paper.pdf)

2016: [Building End-To-End Dialogue Systems Using Generative Hierarchical Neural Network Models](https://arxiv.org/pdf/1507.04808.pdf)

2015: [Generating Sentences from a Continuous Space](https://arxiv.org/pdf/1511.06349.pdf)

## Papers - Transformer era

## Papers - Recent

- [Nougat: Neural Optical Understanding for Academic Documents](https://arxiv.org/pdf/2308.13418v1.pdf)
- [The ConceptARC Benchmark: Evaluating Understanding and Generalization in the ARC Domain](https://arxiv.org/pdf/2305.07141.pdf)

