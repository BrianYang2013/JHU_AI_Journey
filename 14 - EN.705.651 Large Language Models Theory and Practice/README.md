# About this course

## Course



## General

## Textbook

## Grade

1. 30%: Paper Presentations/Discussion Leadership
2. 30%: Preparation and Participation 
3. 40%: Course Project

# Course Topics

 1. Introduction: The Structure, Statistics, and Representation of Language
2. Introduction to the Modeling of Language
3. What Is Attention?
4. The Transformer: All You Need Is Attention
5. The First Large Language Models -- Finally, Good Text Generation!
6. GPT-3, Meta-Learning, and Prompting
7. Prompting, Continued
8. Composability and Chaining
9. ReAct -- Agentive LLM Systems
10. Types of Tuning
11. Efficiency Through Software
12. Efficiency Through Hardware
13. Final Projects Pt. I
14. Final Projects Pt. II

# Check List

Discussion

Office Hours

Course

# Paper and article

## Resources: 

[Anti-hype LLM reading list](https://gist.github.com/veekaybee/be375ab33085102f9027853128dc5f0e)

Awesome

JHU Courses

- [SP23 CS 601.471/671 NLP: Self-supervised Models](https://self-supervised.cs.jhu.edu/sp2023/)
- [FA22 CSCI 601.771: Self-supervised Statistical Models](https://self-supervised.cs.jhu.edu/fa2022/)
- 

## Papers - Before Transformer

2000: [A Neural Probabilistic Language Model](https://proceedings.neurips.cc/paper_files/paper/2000/file/728f206c2a01bf572b5940d7d9a8fa4c-Paper.pdf)

2015: [From Feedforward to Recurrent LSTM Neural Networks for Language Modeling](https://proceedings.neurips.cc/paper_files/paper/2000/file/728f206c2a01bf572b5940d7d9a8fa4c-Paper.pdf)

2016: [Building End-To-End Dialogue Systems Using Generative Hierarchical Neural Network Models](https://arxiv.org/pdf/1507.04808.pdf)

2015: [Generating Sentences from a Continuous Space](https://arxiv.org/pdf/1511.06349.pdf)

## Papers - Foundation in Transformer era

- [Attention is all you Need](https://arxiv.org/abs/1706.03762)
- [Scaling Laws for Neural Language Models](https://arxiv.org/abs/2001.08361)
- [BERT](https://arxiv.org/abs/1810.04805)
- [Training Language Models to Follow Instructions](https://arxiv.org/abs/2203.02155)
- [Language Models are Few-Shot Learners](https://arxiv.org/abs/2005.14165)
- GPT:
  - Improving language understanding by generative pre-training
  - [Language Models are Unsupervised Multi-Task Learners](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)
  - [Language models are few-shot learners](https://arxiv.org/abs/2005.14165)

## Papers - Recent

- [Nougat: Neural Optical Understanding for Academic Documents](https://arxiv.org/pdf/2308.13418v1.pdf)
- [The ConceptARC Benchmark: Evaluating Understanding and Generalization in the ARC Domain](https://arxiv.org/pdf/2305.07141.pdf)

